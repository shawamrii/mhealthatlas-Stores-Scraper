version: '3'
services:
  mhealthatlas-frontend-web:
    build: ./frontendWeb/
    hostname: mhealthatlas-frontend-web
    env_file:
      - ./frontendWeb/.env
    ports:
      - "5556:5556"
    volumes:
      - ./ssl/frontend/frontend.pem:/etc/nginx/certs/frontend.pem
      - ./ssl/frontend/frontend-key.pem:/etc/nginx/certs/frontend-key.pem
    depends_on:
      - mhealthatlas-gateway
      - keycloak-mhealthatlas

  mhealthatlas-apps-service:
    build: ./apps/
    hostname: mhealthatlas-apps-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./apps/.env
    # ports:
    #   - "5551:5551"
    volumes:
      - ./ssl/app/truststore.jks:/ssl/truststore.jks
      - ./ssl/app/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-apps-db
      - kafka1
      - keycloak-mhealthatlas

  mhealthatlas-rating-service:
    build: ./rating/
    hostname: mhealthatlas-rating-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./rating/.env
    # ports:
    #   - "5552:5552"
    volumes:
      - ./ssl/rating/truststore.jks:/ssl/truststore.jks
      - ./ssl/rating/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-ratings-db
      - kafka1
      - keycloak-mhealthatlas

  mhealthatlas-taxonomy-service:
    build: ./taxonomy/
    hostname: mhealthatlas-taxonomy-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./taxonomy/.env
    # ports:
    #   - "5553:5553"
    volumes:
      - ./ssl/taxonomy/truststore.jks:/ssl/truststore.jks
      - ./ssl/taxonomy/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-taxonomies-db
      - kafka1
      - keycloak-mhealthatlas

  mhealthatlas-user-management-service:
    build: ./user-management/
    hostname: mhealthatlas-user-management-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./user-management/.env
    # ports:
    #   - "5554:5554"
    volumes:
      - ./ssl/user/truststore.jks:/ssl/truststore.jks
      - ./ssl/user/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-user-management-db
      - kafka1
      - keycloak-mhealthatlas

  enterprise-management-service:
    build: ./enterprise-management/
    hostname: enterprise-management-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./enterprise-management/.env
    # ports:
    #   - "5557:5557"
    volumes:
      - ./ssl/enterprise/truststore.jks:/ssl/truststore.jks
      - ./ssl/enterprise/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-enterprise-management-db
      - kafka1
      - keycloak-mhealthatlas

  enterprise-apps-service:
    build: ./enterprise-apps/
    hostname: enterprise-apps-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./enterprise-apps/.env
    # ports:
    #   - "5559:5559"
    volumes:
      - ./ssl/enterprise-app/truststore.jks:/ssl/truststore.jks
      - ./ssl/enterprise-app/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-enterprise-apps-db
      - kafka1
      - keycloak-mhealthatlas

  mhealthatlas-users-service:
    build: ./mhealthatlas-users/
    hostname: mhealthatlas-users-service
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./mhealthatlas-users/.env
    # ports:
    #   - "5558:5558"
    volumes:
      - ./ssl/mhealthatlas-user/truststore.jks:/ssl/truststore.jks
      - ./ssl/mhealthatlas-user/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-mhealthatlas-users-db
      - kafka1
      - keycloak-mhealthatlas

  aqe-mapping-service:
    build: ./AqeMapping/
    hostname: aqe-mapping-service
    labels:
      collect_logs_with_filebeat: "false"
      decode_log_event_to_json_object: "false"
    env_file:
      - ./AqeMapping/.env
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-aqe-mapping-db
      - kafka1
      - keycloak-mhealthatlas

  store-apps-service:
    build: ./StoreApps/
    hostname: store-apps-service
    labels:
      collect_logs_with_filebeat: "false"
      decode_log_event_to_json_object: "false"
    env_file:
      - ./StoreApps/.env
    #ports:
    #-"5550:5550"
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - postgres-store-apps-db
      - kafka1
      - keycloak-mhealthatlas


  mhealthatlas-gateway:
    build: ./gateway/
    hostname: mhealthatlas-gateway
    labels:
      collect_logs_with_filebeat: "true"
      decode_log_event_to_json_object: "true"
    env_file:
      - ./gateway/.env
    #scale: 1
    ports:
      - "5555:5555"
      #- "5555"
    volumes:
      - ./ssl/gateway/truststore.jks:/ssl/truststore.jks
      - ./ssl/gateway/keystore.jks:/ssl/keystore.jks
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - keycloak-mhealthatlas

  postgres-mhealthatlas-apps-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-apps-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_APPS_DB_PW}
      - POSTGRES_DB=mhealthatlasApps
    ports:
    # don't expose ports in production
      - "5501:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasAppsDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh
  postgres-store-apps-db:
    image: postgres:12.3-alpine
    container_name: store-apps-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_STORE_APPS_DB_PW}
      - POSTGRES_DB=storeApps
    ports:
    # don't expose ports in production
      - "5500:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initStoreAppsDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh


  postgres-mhealthatlas-ratings-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-ratings-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_RATINGS_DB_PW}
      - POSTGRES_DB=mhealthatlasRatings
    ports:
    # don't expose ports in production
      - "5502:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasRatingsDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-mhealthatlas-taxonomies-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-taxonomies-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_TAXONOMIES_DB_PW}
      - POSTGRES_DB=mhealthatlasTaxonomies
    ports:
    # don't expose ports in production
      - "5503:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasTaxonomiesDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-mhealthatlas-user-management-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-user-management-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_USER_MANAGEMENT_DB_PW}
      - POSTGRES_DB=mhealthatlasUserManagement
    ports:
    # don't expose ports in production
      - "5504:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasUserManagementDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-enterprise-management-db:
    image: postgres:12.3-alpine
    container_name: enterprise-management-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_ENTERPRISE_MANAGEMENT_DB_PW}
      - POSTGRES_DB=enterpriseManagement
    ports:
    # don't expose ports in production
      - "5505:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initEnterpriseManagementDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-mhealthatlas-users-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-users-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_USERS_DB_PW}
      - POSTGRES_DB=mhealthatlasUsers
    ports:
    # don't expose ports in production
      - "5508:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasUsersDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-aqe-mapping-db:
    image: postgres:12.3-alpine
    container_name: aqe-mapping-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_AQE_MAPPING_DB_PW}
      - POSTGRES_DB=aqeMapping
    ports:
    # don't expose ports in production
      - "5507:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initApplicationQuestionExpertMappingDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-mhealthatlas-enterprise-apps-db:
    image: postgres:12.3-alpine
    container_name: mhealthatlas-enterprise-apps-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_ENTERPRISE_APPS_DB_PW}
      - POSTGRES_DB=mhealthatlasEnterpriseApps
    ports:
    # don't expose ports in production
      - "5509:5432"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./Database/scripts/initMHealthAtlasEnterpriseAppsDB.sql:/docker-entrypoint-initdb.d/init.sql
      - ./Database/scripts/initOutboxSettings.sh:/docker-entrypoint-initdb.d/init.sh

  postgres-keycloak-db:
    image: postgres:12.3-alpine
    container_name: keycloak-db
    labels:
      collect_postgresql_logs_with_filebeat: "true"
    environment:
      - POSTGRES_DB=keycloak
      - POSTGRES_USER=${POSTGRES_MHEALTHATLAS_KEYCLOAK_DB_USER}
      - POSTGRES_PASSWORD=${POSTGRES_MHEALTHATLAS_KEYCLOAK_DB_PW}
    networks:
      - mhealthatlas-backend-network

  keycloak-proxy:
    #build: ./nginx/
    image: nginx:1.19.1-alpine
    container_name: keycloak-proxy
    env_file:
      - ./KeycloakProxy/.env
    ports:
      - "5506:5506"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./KeycloakProxy/keycloak-proxy.conf.template:/etc/nginx/templates/keycloack-proxy.conf.template
      - ./ssl/keycloak_proxy/keycloak_proxy.pem:/etc/nginx/certs/keycloakProxy.crt
      - ./ssl/keycloak_proxy/keycloak_proxy-key.pem:/etc/nginx/certs/keycloakProxy.key
    depends_on:
      - keycloak-mhealthatlas

  keycloak-mhealthatlas:
    build: ./Keycloak/
    container_name: mhealthatlas-keycloak
    environment:
      - DB_VENDOR=postgres
      - DB_ADDR=keycloak-db
      - DB_USER=${POSTGRES_MHEALTHATLAS_KEYCLOAK_DB_USER}
      - DB_PASSWORD=${POSTGRES_MHEALTHATLAS_KEYCLOAK_DB_PW}
      - KEYCLOAK_USER=${KEYCLOAK_MHEALTHATLAS_USERNAME}
      - KEYCLOAK_PASSWORD=${KEYCLOAK_MHEALTHATLAS_PASSWORD}
      - KEYCLOAK_DEFAULT_THEME=custom
      - PROXY_ADDRESS_FORWARDING=true
      - X509_CA_BUNDLE=/etc/x509/https/cacert.crt
      - KAFKA_SERVER=${KAFKA_SERVER}
    command:
      - "-Dkeycloak.migration.action=import"
      - "-Dkeycloak.migration.provider=singleFile"
      - "-Dkeycloak.migration.file=/tmp/Realm.json"
      - "-Dkeycloak.migration.strategy=OVERWRITE_EXISTING"
    ports:
    # don't expose ports in production
      - "8443:8443"
    networks:
      - mhealthatlas-backend-network
    volumes:
        - ./Keycloak/MHealthAtlas-realm.json:/tmp/Realm.json
        - ./Keycloak/themes/Custom/:/opt/jboss/keycloak/themes/custom
        - ./ssl/keycloak/keycloak.pem:/etc/x509/https/tls.crt
        - ./ssl/keycloak/keycloak-key.pem:/etc/x509/https/tls.key
        - ./ssl/cacert.pem:/etc/x509/https/cacert.crt
    depends_on:
      - postgres-keycloak-db

  consul1:
    image: "consul"
    container_name: "consul1"
    environment:
      - CONSUL_BIND_INTERFACE=eth0
    ports:
      #secure in production
      - "8500:8500"
    command: "agent -server -ui -client=0.0.0.0 -bind='{{ GetInterfaceIP \"eth0\" }}' -bootstrap-expect=3"
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./ServiceDiscovery/consul-server-config-0.json:/consul/config/consul-config.json
      - ./ssl/cacert.pem:/consul/config/ssl/consul-agent-ca.pem
      - ./ssl/consul1/consul1.pem:/consul/config/ssl/dc1-server-consul-0.pem
      - ./ssl/consul1/consul1-key.pem:/consul/config/ssl/dc1-server-consul-0-key.pem

  consul2:
    image: "consul"
    container_name: "consul2"
    command: "agent -server -retry-join consul1"
    depends_on:
      - consul1
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./ServiceDiscovery/consul-server-config-1.json:/consul/config/consul-config.json
      - ./ssl/cacert.pem:/consul/config/ssl/consul-agent-ca.pem
      - ./ssl/consul2/consul2.pem:/consul/config/ssl/dc1-server-consul-1.pem
      - ./ssl/consul2/consul2-key.pem:/consul/config/ssl/dc1-server-consul-1-key.pem

  consul3:
    image: "consul"
    container_name: "consul3"
    command: "agent -server -retry-join consul1"
    depends_on:
      - consul1
    networks:
      - mhealthatlas-backend-network
    volumes:
      - ./ServiceDiscovery/consul-server-config-2.json:/consul/config/consul-config.json
      - ./ssl/cacert.pem:/consul/config/ssl/consul-agent-ca.pem
      - ./ssl/consul3/consul3.pem:/consul/config/ssl/dc1-server-consul-2.pem
      - ./ssl/consul3/consul3-key.pem:/consul/config/ssl/dc1-server-consul-2-key.pem

  elasticsearch:
    privileged: true
    build: ./ElasticSearch/
    container_name: elasticsearch
    user: root
    volumes:
      - ./ElasticSearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ./ElasticSearch/securityconfig/conifg.yml:/usr/share/elasticsearch/plugins/opendistro_security/securityconfig/config.yml
      - ./ElasticSearch/securityconfig/internal_users.yml:/usr/share/elasticsearch/plugins/opendistro_security/securityconfig/internal_users.yml
      - ./ssl/cacert.pem:/usr/share/elasticsearch/config/mediqsoft-root-ca.pem
      - ./ssl/elasticsearch/elasticsearch.pem:/usr/share/elasticsearch/config/es-admin.pem
      - ./ssl/elasticsearch/elasticsearch-key.pem:/usr/share/elasticsearch/config/es-admin-key.pem
    networks:
      - elastic-stack-network
    environment:
      - "discovery.type=single-node"
      - network.host=0.0.0.0
    env_file:
      - ./ElasticSearch/.env

  logstash:
    image: docker.elastic.co/logstash/logstash-oss:7.9.1
    container_name: logstash
    env_file:
      - ./Logstash/.env
    volumes:
      - ./Logstash/pipeline:/usr/share/logstash/pipeline/            # Pipeline configuration
      - ./ssl/cacert.pem:/usr/share/logstash/config/mediqsoft-root-ca.pem
      - ./ssl/logstash/logstash-key.pem:/usr/share/logstash/config/logstash-key.pem
      - ./ssl/logstash/logstash.pem:/usr/share/logstash/config/logstash.pem
    networks:
      - elastic-stack-network
    depends_on:
      - elasticsearch

  kibana:
    build: ./Kibana/
    container_name: kibana
    volumes:
      - ./Kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
      - ./ssl/cacert.pem:/usr/share/kibana/config/mediqsoft-root-ca.pem
      - ./ssl/kibana/kibana-key.pem:/usr/share/kibana/config/kibana-key.pem
      - ./ssl/kibana/kibana.pem:/usr/share/kibana/config/kibana.pem
    ports:
      - "5601:5601"
    networks:
      - elastic-stack-network
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat-oss:7.9.1
    container_name: filebeat
    command: filebeat -e -strict.perms=false
    env_file:
      - ./Filebeat/.env
    user: root
    volumes:
      - ./Filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:rw # Configuration file
      - /var/lib/docker/containers:/var/lib/docker/containers:ro           # Docker logs
      - /var/run/docker.sock:/var/run/docker.sock:ro                       # Additional information about containers
      - ./ssl/cacert.pem:/usr/share/filebeat/config/mediqsoft-root-ca.pem
      - ./ssl/filebeat/filebeat-key.pem:/usr/share/filebeat/config/filebeat-key.pem
      - ./ssl/filebeat/filebeat.pem:/usr/share/filebeat/config/filebeat.pem
    networks:
      - elastic-stack-network
    depends_on:
      - logstash

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    hostname: jaeger
    ports:
      - '5775:5775/udp'
      - '6831:6831/udp'
      - '6832:6832/udp'
      - '5778:5778'
      - '16686:16686'
      - '14268:14268'
      - '9411:9411'
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=9411
    networks:
      - mhealthatlas-backend-network

  zookeeper:
    image: 'bitnami/zookeeper:latest'
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - '2181:2181'
    environment:
      - ZOO_SERVER_ID=1
      - ZOO_PORT_NUMBER=2181
      - ZOO_SERVERS=server.1=zookeeper:2888:3888
      - ZOO_4LW_COMMANDS_WHITELIST=dump, srvr, mntr
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - mhealthatlas-backend-network

  kafka1:
    image: 'bitnami/kafka:latest'
    hostname: kafka1
    container_name: kafka1
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka1:19092,EXTERNAL://kafka1:9092
      - KAFKA_CFG_LISTENERS=INTERNAL://kafka1:19092,EXTERNAL://kafka1:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_BROKER_ID=1
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    networks:
      - mhealthatlas-backend-network

  kafka2:
    image: 'bitnami/kafka:latest'
    hostname: kafka2
    container_name: kafka2
    ports:
      - '9093:9093'
    environment:
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka2:19093,EXTERNAL://kafka2:9093
      - KAFKA_CFG_LISTENERS=INTERNAL://kafka2:19093,EXTERNAL://kafka2:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_BROKER_ID=2
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    networks:
      - mhealthatlas-backend-network

  kafka3:
    image: 'bitnami/kafka:latest'
    hostname: kafka3
    container_name: kafka3
    ports:
      - '9094:9094'
    environment:
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka3:19094,EXTERNAL://kafka3:9094
      - KAFKA_CFG_LISTENERS=INTERNAL://kafka3:19094,EXTERNAL://kafka3:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_BROKER_ID=3
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    networks:
      - mhealthatlas-backend-network

  app-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: app-debezium-connect
    hostname: app-debezium-connect
    ports:
      - '8083:8083'
    environment:
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: app-debezium_connect_config
      OFFSET_STORAGE_TOPIC: app-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: app-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-mhealthatlas-apps-db

  taxonomy-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: taxonomy-debezium-connect
    hostname: taxonomy-debezium-connect
    ports:
      - '8084:8083'
    environment:
      GROUP_ID: 2
      CONFIG_STORAGE_TOPIC: taxonomy-debezium_connect_config
      OFFSET_STORAGE_TOPIC: taxonomy-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: taxonomy-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-mhealthatlas-taxonomies-db

  aqe-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: aqe-debezium-connect
    hostname: aqe-debezium-connect
    ports:
      - '8085:8083'
    environment:
      GROUP_ID: 3
      CONFIG_STORAGE_TOPIC: aqe-debezium_connect_config
      OFFSET_STORAGE_TOPIC: aqe-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: aqe-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-aqe-mapping-db

  rating-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: rating-debezium-connect
    hostname: rating-debezium-connect
    ports:
      - '8086:8083'
    environment:
      GROUP_ID: 4
      CONFIG_STORAGE_TOPIC: rating-debezium_connect_config
      OFFSET_STORAGE_TOPIC: rating-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: rating-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-mhealthatlas-ratings-db

  user-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: user-debezium-connect
    hostname: user-debezium-connect
    ports:
      - '8087:8083'
    environment:
      GROUP_ID: 5
      CONFIG_STORAGE_TOPIC: user-debezium_connect_config
      OFFSET_STORAGE_TOPIC: user-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: user-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-mhealthatlas-user-management-db
  store-debezium-connect:
    build: ./DebeziumTransformer/
    container_name: store-debezium-connect
    hostname: store-debezium-connect
    ports:
      - '8088:8083'
    environment:
      GROUP_ID: 6
      CONFIG_STORAGE_TOPIC: store-debezium_connect_config
      OFFSET_STORAGE_TOPIC: store-debezium_connect_offsets
      STATUS_STORAGE_TOPIC: store-debezium_connect_status
      BOOTSTRAP_SERVERS: kafka1:9092
      ZOOKEEPER_CONNECTION: zookeeper:2181
    networks:
      - mhealthatlas-backend-network
    depends_on:
      - kafka1
      - postgres-store-apps-db


  kafka-setup:
    image: confluentinc/cp-kafka:5.2.7
    hostname: kafka-setup
    container_name: kafka-setup
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
                        cub kafka-ready -b kafka1:9092 3 20 && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic android-application && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic ios-application && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic private-application && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic rating && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic user-management && \
                        kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 3 --replication-factor 3 --topic enterprise-management && \
                        echo Waiting 60 seconds for Connect to be ready... && \
                        sleep 60 && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://app-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_app.config && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://taxonomy-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_taxonomy.config && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://aqe-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_aqe.config && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://rating-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_rating.config && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://store-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_store.config && \
                        curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://user-debezium-connect:8083/connectors/ -d @/opt/docker/connectors/connector_user.config'"
    environment:
      # The following settings are listed here only to satisfy the image's requirements.
      # We override the image's `command` anyways, hence this container will not start a broker.
      KAFKA_BROKER_ID: ignored
      KAFKA_ZOOKEEPER_CONNECT: ignored
    volumes:
      - ./DebeziumTransformer/connector_app.config:/opt/docker/connectors/connector_app.config
      - ./DebeziumTransformer/connector_taxonomy.config:/opt/docker/connectors/connector_taxonomy.config
      - ./DebeziumTransformer/connector_aqe.config:/opt/docker/connectors/connector_aqe.config
      - ./DebeziumTransformer/connector_rating.config:/opt/docker/connectors/connector_rating.config
      - ./DebeziumTransformer/connector_user.config:/opt/docker/connectors/connector_user.config
      - ./DebeziumTransformer/connector_store.config:/opt/docker/connectors/connector_store.config

    depends_on:
      - kafka1
    networks:
      - mhealthatlas-backend-network

networks:
  mhealthatlas-backend-network:
    driver: bridge
  elastic-stack-network:
    driver: bridge
